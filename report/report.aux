\relax 
\@writefile{toc}{\contentsline {section}{\numberline {2}Maximum Likelihood Estimation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Emission Parameters Estimation}{3}}
\newlabel{sec:emissions}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Laplace Smoothing}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Sequence Labeling}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Results}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}First-Order Hidden Markov Model}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Transition Parameters Emission}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Viterbi Algorithm}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Initialization}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Recursion}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Backtracking}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Second-Order Hidden Markov Model}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Emissions Matrix Training}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Transition Matrix Training}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Modified Viterbi Algorithm}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Initialization}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Recursion}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Termination}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Backtracing}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Results}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Design Challenge}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Preprocessing}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}String tokenization}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Replacing irrelevant words}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Stop words}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Simple vectorization}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Tokenizing}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}One-hot encoding}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}word2vec}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Skip-gram}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Forward}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Backward}{11}}
\citation{DBLP:journals/corr/MikolovSCCD13}
\citation{pennington2014glove}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Training}{12}}
\@writefile{toc}{\contentsline {paragraph}{Positive sampling}{12}}
\@writefile{toc}{\contentsline {paragraph}{Negative sampling}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}Visualization}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.6}Inference}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces t-SNE visualization of \emph  {word2vec} model on EN dataset\relax }}{13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:word2vec}{{1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Recurrent neural network (RNN)}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Forward}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RNN model architecture \cite  {Goodfellow-et-al-2016}\relax }}{14}}
\newlabel{fig:rnn}{{2}{14}}
\newlabel{eqn:forward-all}{{5.6}{14}}
\newlabel{eqn:forward-h}{{5.7}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Backward}{14}}
\@writefile{toc}{\contentsline {paragraph}{Output layer}{14}}
\newlabel{eqn:backward-o}{{5.8}{14}}
\newlabel{eqn:backward-c}{{5.9}{15}}
\newlabel{eqn:backward-V}{{5.10}{15}}
\@writefile{toc}{\contentsline {paragraph}{Hidden layer}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Computational graph showing gradients for $\bm  {h}^{(n)}$\relax }}{15}}
\newlabel{fig:grad-hn}{{3}{15}}
\newlabel{eqn:backward-hn}{{5.11}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Computational graph showing gradients for $\bm  {h}^{(t)}$\relax }}{16}}
\newlabel{fig:grad-ht}{{4}{16}}
\newlabel{eqn:backward-ht}{{5.12}{16}}
\newlabel{eqn:backward-b}{{5.13}{17}}
\newlabel{eqn:backward-W}{{5.14}{17}}
\newlabel{eqn:backward-U}{{5.15}{17}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{17}}
\newlabel{eqn:backward-all}{{5.16}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Multilayer perceptron (MLP)}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Forward}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces MLP computational graph\relax }}{18}}
\newlabel{fig:mlp}{{5}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Backward}{18}}
\@writefile{toc}{\contentsline {paragraph}{Output layer}{18}}
\@writefile{toc}{\contentsline {paragraph}{Second hidden layer}{18}}
\@writefile{toc}{\contentsline {paragraph}{First hidden layer}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Training}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}Weight initialization}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.2}Batch updates}{19}}
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training loss against epochs\relax }}{20}}
\newlabel{fig:batch-updates}{{6}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Before batch updates}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After batch updates}}}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.3}Optimizers}{20}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic gradient descent}{20}}
\@writefile{toc}{\contentsline {paragraph}{Adagrad}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.4}Regularization}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Distribution of labels in FR dataset\relax }}{20}}
\newlabel{fig:class-labels}{{7}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.5}Class weighting}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.6}Hyperparameter optimization}{21}}
\newlabel{sec:hyperparams}{{5.6.6}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameter Selection\relax }}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Results}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.1}RNN}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces RNN Results\relax }}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces MLP Results\relax }}{22}}
\newlabel{table:mlpresults}{{3}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.2}MLP}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Evaluation}{22}}
\bibstyle{ieeetr}
\bibdata{bibliography}
\bibcite{DBLP:journals/corr/MikolovSCCD13}{1}
\bibcite{pennington2014glove}{2}
\bibcite{Goodfellow-et-al-2016}{3}
\bibcite{srivastava2014dropout}{4}
