{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we extend the HMM to have second-order dependencies by modifying the Viterbi algorithm.\n",
    "\n",
    "The logic is as follows:\n",
    "Our Viterbi algorithm thus far uses a first-order assumption, only including the probability of the previous state in the calculation of the likelihood of the current state. We want to now include the *previous two states* in our calculation.\n",
    "\n",
    "The algorithm can still be designed via dynamic programming. However, runtime is further multiplied by (number of states) due to the additional layer of multiplication, for a total runtime of $O(T \\times |S|^3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import tqdm_notebook as tqdm, tnrange as trange\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emissions\n",
    "We train the emissions matrix by using maximum likelihood estimation:\n",
    "\n",
    "$$ e(x|y) = \\frac{Count(x->y)}{Count(y)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The behavior of the emissions matrix is the same - we use the same function as used in Part 3\n",
    "def learn_emissions(train_filename):\n",
    "    ''' Learns emissions parameters from data and returns them as a nested dictionary '''\n",
    "    with open(train_filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    observations = set()\n",
    "    # Track emission counts\n",
    "    emissions = {} # Where key is y, and value is a dictionary of emissions x from y with their frequency\n",
    "\n",
    "    # Learn from data\n",
    "    for line in tqdm(lines, desc='Emissions'):\n",
    "        data_split = line.strip().rsplit(' ', 1)\n",
    "\n",
    "        # Only process valid lines\n",
    "        if len(data_split) == 2:\n",
    "            obs, state = data_split\n",
    "            observations.add(obs)\n",
    "\n",
    "            # Track this emission\n",
    "            if state in emissions:\n",
    "                current_emissions = emissions[state]\n",
    "            else:\n",
    "                current_emissions = defaultdict(int)\n",
    "                \n",
    "            current_emissions[obs] += 1\n",
    "            emissions[state] = current_emissions # Update\n",
    "    \n",
    "    emission_counts = {k: sum(emissions[k].values()) for k in emissions}\n",
    "    return emissions, emission_counts, observations\n",
    "\n",
    "\n",
    "def get_emission_parameters(emissions, emission_counts, observations, x, y, k=1):\n",
    "    ''' Returns the MLE of the emission parameters based on the emissions dictionary '''\n",
    "    if y not in emissions:  # edge case: no records of emission from this state\n",
    "        return 0\n",
    "    state_data = emissions[y]\n",
    "    count_y = emission_counts[y] #sum(state_data.values()) # Denominator\n",
    "    \n",
    "    if x not in observations:  # edge case: no records of emission of this word\n",
    "        count_y_x = k\n",
    "    else:\n",
    "        count_y_x = state_data[x]\n",
    "    \n",
    "    e = count_y_x / (count_y + k)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_transitions(train_filename):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing (key, value) where\n",
    "        key: (u, v)\n",
    "        value: Count(u, v)\n",
    "    \"\"\"\n",
    "    with open(train_filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    transitions = defaultdict(int)\n",
    "    prev_prev_state = 'PRESTART'\n",
    "    prev_state = 'START'\n",
    "    # avoid excessive indentations\n",
    "    for line in tqdm(lines, desc='Transitions'):\n",
    "        data_split = line.strip().rsplit(' ', 1)\n",
    "        \n",
    "        # line breaks -> new sequence\n",
    "        if len(data_split) < 2:\n",
    "            transitions[(prev_prev_state, prev_state, 'STOP')] += 1\n",
    "            prev_prev_state = 'PRESTART'\n",
    "            prev_state = 'START'\n",
    "            continue\n",
    "\n",
    "        obs, curr_state = data_split\n",
    "        transitions[(prev_prev_state, prev_state, curr_state)] += 1\n",
    "        prev_prev_state = prev_state\n",
    "        prev_state = curr_state\n",
    "        \n",
    "    # count number of 'from' states\n",
    "    transition_counts = defaultdict(int)\n",
    "    for (t, u, v), counts in transitions.items():\n",
    "        transition_counts[(t, u)] += counts\n",
    "\n",
    "    # get all unique states\n",
    "    t, u, v = zip(*transitions)\n",
    "    states = set(t) | set(u) | set(v)\n",
    "    return transitions, transition_counts, states\n",
    "\n",
    "def get_transition_parameters(transitions, transition_counts, t, u, v):\n",
    "    if transition_counts[(t, u)] == 0:  # edge case: no records of transitions starting from u\n",
    "        return 0\n",
    "    return transitions[(t, u, v)] / transition_counts[(t, u)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the Viterbi algorithm for Part 4, to include second-order dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(transitions, transition_counts, states, emissions, emission_counts, observations, obs_seq):\n",
    "    a = lambda prev2, prev, curr: get_transition_parameters(transitions, transition_counts, prev2, prev, curr)\n",
    "    b = lambda state, out: get_emission_parameters(emissions, emission_counts, observations, x=out, y=state)\n",
    "\n",
    "    # create empty tables\n",
    "    n = len(obs_seq) + 3  # PRESTART + START + |obs_seq| + STOP\n",
    "    P = pd.DataFrame(index=states, columns=range(n)).fillna(0)  # probability table\n",
    "    B = pd.DataFrame(index=states, columns=range(n))  # backpointer table\n",
    "    \n",
    "    # initialization\n",
    "    P.loc['PRESTART', 0] = 1\n",
    "    P.loc['START', 1] = 1\n",
    "    \n",
    "    # recursion\n",
    "    for j in range(2, n-1):\n",
    "        x = obs_seq[j-2]  # obs_seq starts from 0, j starts from 1\n",
    "        for v in states:  # curr state\n",
    "            for u in states:  # prev state\n",
    "                for t in states: # prev_2 state\n",
    "                    p = P.loc[u, j-1] * P.loc[t, j-2] * a(t, u, v) * b(v, x)\n",
    "                    if p > P.loc[v, j]:\n",
    "                        P.loc[v, j] = p  # update probability\n",
    "                        B.loc[v, j] = t + ' ' + u  # update backpointer\n",
    "                    \n",
    "    # termination\n",
    "    j = n - 1\n",
    "    v = 'STOP'\n",
    "    for u in states:\n",
    "        for t in states:\n",
    "            p = P.loc[u, j-1] * P.loc[t, j-2] * a(t, u, v)\n",
    "            if p > P.loc[v, j]:\n",
    "                P.loc[v, j] = p  # probability\n",
    "                B.loc[v, j] = t + ' ' + u  # backpointer\n",
    "    # backtrace\n",
    "    # second order viterbi requires backpropagation keep track of the second order item, \n",
    "    # as the limited horizon assumption is now limited to *two* entries instead.\n",
    "    state_seq = ['STOP']\n",
    "    # Skip steps 2 at a time\n",
    "    for i in range(n-1, 0, -2):\n",
    "        curr_state = state_seq[-1]\n",
    "        B_lookup = B.loc[curr_state, i]\n",
    "        if isinstance(B_lookup, str):\n",
    "            prev_state = B.loc[curr_state, i].split(' ')[1]\n",
    "            prev_prev_state = B.loc[curr_state, i].split(' ')[0]\n",
    "        else: # edge case: no possible transition to START\n",
    "            for j in range(int(i/2)+1):\n",
    "                state_seq.append(('O'))\n",
    "                state_seq.append(('O'))\n",
    "            break\n",
    "        state_seq.append(prev_state)\n",
    "        state_seq.append(prev_prev_state)\n",
    "    state_seq = state_seq[::-1][2:-1]  # reverse and drop PRESTART, START, STOP\n",
    "    return P, B, state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset):\n",
    "    \"\"\"\n",
    "    Takes in dataset name, obtains transition and emission parameters.\n",
    "    \"\"\"\n",
    "    train_filename = f'data/{dataset}/train'\n",
    "    # train\n",
    "    t = learn_transitions(train_filename)\n",
    "    e = learn_emissions(train_filename)\n",
    "    return t, e\n",
    "\n",
    "\n",
    "def validate(dataset, t, e):\n",
    "    \"\"\"\n",
    "    Takes in dataset name, and HMM parameters.\n",
    "    Use HMM parameters to estimate state sequence.\n",
    "    Write state sequence to file alongside input observation sequence.\n",
    "    \"\"\"\n",
    "    val_filename = f'data/{dataset}/dev.in'\n",
    "    out_filename = f'data/{dataset}/dev.p4.out'\n",
    "\n",
    "    # validate with train parameters\n",
    "    label_sequence = lambda sentence: viterbi(*t, *e, sentence)\n",
    "    \n",
    "    # read validation file for sequence\n",
    "    with open(val_filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sentence = []\n",
    "    result = []\n",
    "    for word in tqdm(lines, desc=dataset):\n",
    "        if word == '\\n':\n",
    "            _, _, s = label_sequence(sentence)  # ignore tables\n",
    "            result.extend(s)\n",
    "            result.append('\\n')\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(word.strip())\n",
    "            \n",
    "    # write sequence to out file\n",
    "    with open(out_filename, 'w') as f:\n",
    "        for i in range(len(lines)):\n",
    "            word = lines[i].strip()\n",
    "            if word:\n",
    "                pred = result[i]\n",
    "                f.write(word + ' ' + pred)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transitions: 100%|██████████| 11236/11236 [00:00<00:00, 453050.31it/s]\n",
      "Emissions: 100%|██████████| 11236/11236 [00:00<00:00, 1041323.99it/s]\n",
      "EN: 100%|██████████| 1520/1520 [04:06<00:00,  6.12it/s]\n",
      "Transitions: 100%|██████████| 28199/28199 [00:00<00:00, 1683512.61it/s]\n",
      "Emissions: 100%|██████████| 28199/28199 [00:00<00:00, 1408708.65it/s]\n",
      "FR: 100%|██████████| 3700/3700 [01:06<00:00, 57.28it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = ['EN', 'FR']\n",
    "for dataset in datasets:\n",
    "    t, e = train(dataset)\n",
    "    validate(dataset, t, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
